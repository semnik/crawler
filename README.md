# Задача
Задача: написать супер-минималистичный краулер

Мы запускаем скрипт, даём ему начальный URL, говорим в сколько потоков нужно краулить.

После того, как мы остановили скрипт, мы можем из него получить список адресов, которые он посетил (возможно, для этого нужно запустить какой-то скрипт, который нам эти адреса выдаст).

Содержимое страниц сохранять не нужно. Для извлечения адресов из страниц можно использовать любые средства, хоть регулярки (конечно, более уместные средства приветствуются).

Потенциально мы можем захотеть запускать потоки краулера на другой машине (не нужно это реализовывать, но нужно описать, что для этого осталось доделать в решении).

В решении нас в первую очередь интересует минимальная работоспособность, устройство проекта и культура кода.


# Решение

 
Пока успел реализовать лишь минимальную работоспособность с плохим качеством 
кода. 
Я буду считать что защиты от парсинга нет.

После того, как мы остановили скрипт-краулер, мы можем получить список 
адресов в файле parsed_urls.txt запуском скрипта get_parsed_urls.py  .

Принял решение реализовать без очередей т.к. было мало времени на работу 
над этой задачей. Если перейти на очереди, то можно реализовать  
такую архитектуру чтобы была одна очередь на краулинг и воркеры на разных 
машинах
 подключались к этой очереди.


Чтобы использовать несколько ВМ в данной архитектуре можно реализовать запуск 
нескольких 
инстансов (докер образов) , с разными 
конфигами (для каждого инстанса свой конфиг) 
 на 
нескольких 
машинах.


 #TODO 
 добавить первичный ключ int, реализовать проверку есть ли урл в базе перед 
 добавлением , перейти на очередь и Celery
 
#Запуск
Python 3.6

docker-compose up 

python crawler.py

python get_parsed_urls.py.py



